{\rtf1\ansi\ansicpg1252\cocoartf2822
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\fswiss\fcharset0 Helvetica;}
{\colortbl;\red255\green255\blue255;}
{\*\expandedcolortbl;;}
\paperw11900\paperh16840\margl1440\margr1440\vieww11520\viewh8400\viewkind0
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f0\fs24 \cf0 import tensorflow as tf\
from tensorflow import keras\
from tensorflow.keras import layers\
from tensorflow_model_optimization.quantization.keras import quantize_model, quantize_annotate_layer, quantize_annotate_model, quantize_scope, quantize_apply\
from tensorflow.keras.datasets import mnist\
import tensorflow_model_optimization as tfmot\
\
# Load and preprocess data\
(x_train, y_train), (x_test, y_test) = mnist.load_data()\
x_train = x_train.astype("float32") / 255.0\
x_test = x_test.astype("float32") / 255.0\
\
# Build base model\
def build_model():\
    model = keras.Sequential([\
        layers.Flatten(input_shape=(28, 28)),\
        layers.Dense(128, activation='relu'),\
        layers.Dense(64, activation='relu'),\
        layers.Dense(10, activation='softmax')\
    ])\
    return model\
\
base_model = build_model()\
base_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\
\
# Pre-train base model (optional but recommended)\
base_model.fit(x_train, y_train, epochs=3, batch_size=128, validation_split=0.1)\
\
# Apply quantization aware training (QAT)\
quantize_model = tfmot.quantization.keras.quantize_model\
\
qat_model = quantize_model(base_model)\
\
# Compile QAT model\
qat_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\
\
# Fine-tune with QAT\
qat_model.fit(x_train, y_train, batch_size=128, epochs=3, validation_split=0.1)\
\
# Evaluate\
_, test_acc = qat_model.evaluate(x_test, y_test)\
print(f"QAT model test accuracy: \{test_acc:.4f\}")\
\
# Convert to TFLite\
converter = tf.lite.TFLiteConverter.from_keras_model(qat_model)\
converter.optimizations = [tf.lite.Optimize.DEFAULT]\
tflite_quant_model = converter.convert()\
\
# Save the TFLite model\
with open("qat_mnist_model.tflite", "wb") as f:\
    f.write(tflite_quant_model)\
}